{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00516676-348b-4789-94f8-d3607b123c90",
   "metadata": {},
   "source": [
    "# PEATMAP NETWORK TRAINING NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890263b-8a05-44bb-9aea-ed7c9eed8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save code to exec_peatnet.py\n",
    "#%%writefile exec_peatnet_2.0.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from peatnet import *\n",
    "from utils import *\n",
    "from libCarbonFootprint import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9aae4b",
   "metadata": {},
   "source": [
    "## Training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f449ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%writefile` not found (But cell magic `%%writefile` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "# save code to exec_peatnet.py\n",
    "%%writefile exec_peatnet_2.0.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from peatnet import *\n",
    "from utils import *\n",
    "from libCarbonFootprint import *\n",
    "\n",
    "\n",
    "\n",
    "learn_rate = 0.001          # Learning rate\n",
    "num_epochs = 5              # Number of epochs\n",
    "nb_file2merge = 5           # Number of files to merge\n",
    "frac_samples = 0.10         # Fraction of the data to extract\n",
    "normalize = False            # Normalize the data\n",
    "verbose = True              # Verbose mode\n",
    "\n",
    "model_dir = \"../peatnet_models\"\n",
    "\n",
    "\n",
    "carbon_estimation = True    # Estimate the carbon footprint\n",
    "carbon_log_file = \"carbon_footprint.log\"\n",
    "training_log_file = \"peatnet_training\"\n",
    "\n",
    "data_dir = \"/home/gsainton/CALER/PEATMAP/1_NN_training/training_data\" if os.uname().nodename == 'ares6' else \"/data/gsainton/PEATLAND_DATA\"\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(os.path.join(f\"training_log_file_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.log\")),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if verbose:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def get_gpu_usage():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE,\n",
    "                                stderr=subprocess.PIPE, text=True)\n",
    "        return result.stdout\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to execute nvidia-smi: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def is_gpu_free(gpu_id):\n",
    "    gpu_usage = get_gpu_usage()\n",
    "    logger.info(f\"GPU usage:\\n{gpu_usage}\")\n",
    "    # Check for the presence of any process using the specified GPU\n",
    "    if f' No running processes found' in gpu_usage.split('GPU')[gpu_id + 1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def setup_device(mydevice: str) -> torch.device:\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.error(\"GPU is not available -> device = 'cpu'...\")\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        logger.info(\"GPU found...\")\n",
    "        logger.info(f\"Number of GPUs: {num_gpus}\")\n",
    "        for i in range(num_gpus):\n",
    "            logger.info(f\"Device {i} name: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "        valid_devices = [f'cuda:{i}' for i in range(num_gpus)]\n",
    "        logger.info(f\"Valid GPU references: {valid_devices}\")\n",
    "\n",
    "        # Check if the selected GPU is free\n",
    "        if mydevice in valid_devices:\n",
    "            gpu_id = int(mydevice.split(':')[1])\n",
    "            if is_gpu_free(gpu_id):\n",
    "                logger.info(f\"Using GPU - {mydevice}\")\n",
    "                device = torch.device(mydevice)\n",
    "            else:\n",
    "                logger.error(f\"GPU {mydevice} may be currently in use...\")\n",
    "                device = torch.device(mydevice)\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Invalid GPU reference: {mydevice}. Valid references: {valid_devices}. Exiting...\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    return device\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Get argument from the command line\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network to predict peatland')\n",
    "    parser.add_argument('--num_epochs', type=int, default=2, help='Number of epochs')\n",
    "    parser.add_argument('--nb_file2merge', type=int, default=2, help='Number of files to merge')\n",
    "    parser.add_argument('--frac_samples', type=float, default=0.10, help='Fraction of the data to extract')\n",
    "    parser.add_argument('--gpu_ref', type=str, default='cuda:0', help='GPU reference')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    num_epochs = args.num_epochs\n",
    "    nb_file2merge = args.nb_file2merge\n",
    "    frac_samples = args.frac_samples\n",
    "    mydevice = torch.device(args.gpu_ref)\n",
    "\n",
    "    if frac_samples > 1 or frac_samples < 0:\n",
    "        raise ValueError(\"frac_samples must be between 0 and 1\")\n",
    "\n",
    "    # Exemple of command line:\n",
    "    # python exec_peatnet.py --num_epochs 2 --nb_file2merge 2 --frac_samples 0.10\n",
    "\n",
    "    carbon_log_dir = \"/home/gsainton/CARBON_LOG\" if os.uname().nodename == 'ares6' else \"/obs/gsainton/PEATLAND_DATA\"\n",
    "\n",
    "    if carbon_estimation:\n",
    "        start = datetime.datetime.now()\n",
    "    if not os.path.exists(carbon_log_dir):\n",
    "\n",
    "        os.makedirs(carbon_log_dir)\n",
    "\n",
    "    device = setup_device(mydevice)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    #\n",
    "    # Load and preprocess the data\n",
    "    #\n",
    "    #---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    peatmat_data_proc = PeatNetDataProc(data_dir=data_dir, frac_samples=frac_samples)\n",
    "\n",
    "    peatmat_data_proc.set_list_rdn_files(nb_file2merge)\n",
    "    sub_sampled_data = peatmat_data_proc.get_list_rdn_files()\n",
    "\n",
    "    X, y = peatmat_data_proc.load_data()\n",
    "\n",
    "    logging.info(\"Number of tiles to merge : {}\".format(nb_file2merge))\n",
    "    logging.info(\"Fraction of samples to extract : {}\".format(frac_samples))\n",
    "\n",
    "    X_fields = ['dist0005', 'dist0100', 'dist1000', 'hand0005',\n",
    "        'hand0100', 'hand1000', 'slope',\n",
    "        'elevation', 'wtd', 'landsat_1', 'landsat_2',\n",
    "        'landsat_3', 'landsat_4', 'landsat_5', 'landsat_6',\n",
    "        'landsat_7', 'NDVI']\n",
    "    # Après reflexion, j'ai enlevé les deux colonnes latS et lonS qui de mon\n",
    "    # point de vue ne doivent pas être utilisées pour la prédiction de la\n",
    "    # présence de tourbière\n",
    "\n",
    "    X.columns = X_fields\n",
    "    y_fields = ['peatland']\n",
    "    y.columns = y_fields\n",
    "\n",
    "    if normalize:\n",
    "        logger.info(\"Normalizing the data...\")\n",
    "        fields_to_transform = [ 'dist0005', 'dist0100', 'dist1000', 'hand0005',\n",
    "        'hand0005', 'hand0100', 'hand1000', 'slope', 'wtd',\n",
    "        'landsat_1', 'landsat_2', 'landsat_3', 'landsat_4',\n",
    "        'landsat_7', 'NDVI']\n",
    "        X = peatmat_data_proc.normalize_data(X, fields_to_transform)\n",
    "\n",
    "    # Split the data into train, validation and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                        random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=42)\n",
    "\n",
    "    logger.info(\"Data splitted into train, validation and test datasets\")\n",
    "    logger.info(f\"Train dataset size: {len(X_train)}\")\n",
    "    logger.info(f\"Validation dataset size:sub_sampled_data =  {len(X_val)}\")\n",
    "    logger.info(f\"Test dataset size: {len(X_test)}\")\n",
    "\n",
    "    # Define model parameters\n",
    "    input_size = list(X_train.shape)[1]\n",
    "    output_size = list(y_train.shape)[1] if len(y_train.shape) > 1 else 1\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Create TensorDataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    validate_loader = Data.DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Free some memories\n",
    "    del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    del X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    #\n",
    "    # Define and train the model\n",
    "    #\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    model = PeatNet(input_size, output_size).to(device)\n",
    "\n",
    "    logger.info(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    logger.debug(model)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate,\n",
    "                                 weight_decay=1e-5)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    logger.info(\"Training the model...\")\n",
    "    total_time = train_model(model, train_loader, validate_loader,\n",
    "                            criterion, optimizer, num_epochs=num_epochs,\n",
    "                            device=device)\n",
    "\n",
    "\n",
    "    train_model(model, train_loader, validate_loader, criterion,\n",
    "                optimizer, num_epochs=10, device='cuda', scheduler=scheduler)\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    fullname_model = save_model(model, model_dir, current_time)\n",
    "    logger.info(f\"Model saved to {fullname_model}\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    #\n",
    "    # Test the model after training\n",
    "    #\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    logger.info(\"Testing the model...\")\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(validate_loader, desc='Final Validation',\n",
    "                                    leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(validate_loader)\n",
    "    logger.info(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    #\n",
    "    # Naive carbon footprint estimation\n",
    "    #\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    if carbon_estimation:\n",
    "        carbon_footprint_calculator = CarbonFootprintCalculator(device)\n",
    "        carbon_logger = CarbonFootprintLogger(carbon_log_dir, carbon_log_file)\n",
    "        end, total_energy_kwh, total_carbon_footprint = carbon_footprint_calculator.calculate(start)\n",
    "        carbon_logger.log_carbon_footprint(end, total_energy_kwh,\n",
    "                                           total_carbon_footprint)\n",
    "\n",
    "    logger.info(\"End of the script\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e939b3c",
   "metadata": {},
   "source": [
    "## Test of the program \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb78628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c15065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/gsainton/CALER/PEATMAP/1_NN_training/training_data\" if os.uname().nodename == 'ares6' else \"/data/gsainton/PEATLAND_DATA\"\n",
    "\n",
    "frac_samples = 0.10\n",
    "peatmat_data_proc = PeatNetDataProc(data_dir=data_dir, frac_samples=frac_samples)\n",
    "\n",
    "nb_file2merge = 5\n",
    "\n",
    "peatmat_data_proc.set_list_rdn_files(nb_file2merge)\n",
    "sub_sampled_data = peatmat_data_proc.get_list_rdn_files()\n",
    "\n",
    "def get_lat_lon_from_filename(filename):\n",
    "\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "\n",
    "    filename = filename.split(\"_\")[-1]\n",
    "    # remove .mat extension\n",
    "    filename = filename.split(\".\")[0]\n",
    "    # find \"n\"\n",
    "    n = filename.find(\"n\")\n",
    "    s = filename.find(\"s\")\n",
    "    e = filename.find(\"e\")\n",
    "    w = filename.find(\"w\")\n",
    "\n",
    "    if n != -1:\n",
    "        if e != -1:\n",
    "            lat = int(filename[n+1:e])\n",
    "            lon = int(filename[e+1:])\n",
    "        else :\n",
    "            lat = int(filename[n+1:w])\n",
    "            lon = int(filename[w+1:])*-1\n",
    "    else:\n",
    "        if e != -1:\n",
    "            lat = int(filename[s+1:e])*-1\n",
    "            lon = int(filename[e+1:])\n",
    "        else :\n",
    "            lat = int(filename[s+1:w])*-1\n",
    "            lon = int(filename[w+1:])*-1\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "def get_list_lat_lon_from_filename(filename):\n",
    "\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "\n",
    "    for f in filename:\n",
    "        lat, lon = get_lat_lon_from_filename(f)\n",
    "        lat_list.append(lat)\n",
    "        lon_list.append(lon)\n",
    "\n",
    "    return pd.DataFrame({\"lat\": lat_list, \"lon\": lon_list, \"filename\": filename})\n",
    "\n",
    "tiles_catalog = get_list_lat_lon_from_filename(peatmat_data_proc.list_all_files)\n",
    "\n",
    "display(tiles_catalog)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium # the folium library\n",
    "world_map = folium.Map(zoom_start=10)\n",
    "\n",
    "# Iterator for colors\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', 'darkblue',\n",
    "          'darkgreen', 'cadetblue', 'darkpurple', 'white', 'pink', 'lightblue',\n",
    "          'lightgreen', 'black', 'lightgray', 'beige']\n",
    "\n",
    "# loop over element of the dataframe\n",
    "for i, row in tiles_catalog.iterrows():\n",
    "    # Get the min and max values of the Latitude and Longitude\n",
    "    min_lat = row['lat']\n",
    "    max_lat = row['lat']+5\n",
    "    min_lon = row['lon']\n",
    "    max_lon = row['lon']+5\n",
    "    # Get the color\n",
    "    color = colors[i]\n",
    "    # Create a rectangle\n",
    "    folium.Rectangle(bounds=[[min_lat, min_lon], [max_lat, max_lon]],\n",
    "                    color='purple', fill=True, tooltip= row[\"filename\"],\n",
    "                    name=row[\"filename\"]).add_to(world_map)\n",
    "world_map.fit_bounds(world_map.get_bounds())\n",
    "\n",
    "\n",
    "\n",
    "tiles_rdn_catalog = get_list_lat_lon_from_filename(sub_sampled_data)\n",
    "\n",
    "\n",
    "# Overplot rectangle of the sub_sampled_data on the map\n",
    "for i, row in tiles_rdn_catalog.iterrows():\n",
    "    # Get the min and max values of the Latitude and Longitude\n",
    "    min_lat = row['lat']\n",
    "    max_lat = row['lat']+5\n",
    "    min_lon = row['lon']\n",
    "    max_lon = row['lon']+5\n",
    "    # Get the color\n",
    "    color = colors[i]\n",
    "    # Create a rectangle\n",
    "    folium.Rectangle(bounds=[[min_lat, min_lon], [max_lat, max_lon]],\n",
    "                    color=\"red\", fill=True, line_join=\"round\", popup=\"Set for training\",\n",
    "                    dash_array=\"5, 5\", fill_color=\"red\", fill_opacity=0.5,\n",
    "                    name=row[\"filename\"]).add_to(world_map)\n",
    "\n",
    "display(world_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c571e616",
   "metadata": {},
   "source": [
    "\n",
    "## Applying the model to a complete image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from peatnet import *\n",
    "from utils import *\n",
    "from libCarbonFootprint import *\n",
    "\n",
    "normalize = False            # Normalize the data\n",
    "model_dir = \"../peatnet_models\"\n",
    "input_size = 17\n",
    "output_size = 1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Apply the model to a complete tile\n",
    "fullname_model = \"../peatnet_models/model_peatnet_20240710-155928.ckpt\"\n",
    "# Load the model\n",
    "model = PeatNet(input_size, output_size).to(device)\n",
    "model.load_state_dict(torch.load(fullname_model))\n",
    "model.eval()\n",
    "\n",
    "file_to_predict = np.random.choice(list(set(peatmat_data_proc.list_all_files) - set(sub_sampled_data)),1, replace=False)[0]\n",
    "print(f\"File to predict: {file_to_predict}\")\n",
    "\n",
    "# Load the data\n",
    "X, y = peatmat_data_proc.load_dataset_mat(file_to_predict, outfmt=\"pandas\", with_coord=True)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "X_fields = ['dist0005', 'dist0100', 'dist1000', 'hand0005',\n",
    "    'hand0100', 'hand1000', 'slope',\n",
    "    'elevation', 'wtd', 'landsat_1', 'landsat_2',\n",
    "    'landsat_3', 'landsat_4', 'landsat_5', 'landsat_6',\n",
    "    'landsat_7', 'NDVI', \"lat\", \"lon\"]\n",
    "\n",
    "X.columns = X_fields\n",
    "\n",
    "# Keep lat and lon appart from X\n",
    "lat = X[\"lat\"]\n",
    "lon = X[\"lon\"]\n",
    "X = X.drop(columns=[\"lat\", \"lon\"])\n",
    "\n",
    "y_fields = ['peatland']\n",
    "# Normalize the data\n",
    "y.columns = y_fields\n",
    "\n",
    "fields_to_transform = [ 'dist0005', 'dist0100', 'dist1000', 'hand0005',\n",
    "        'hand0005', 'hand0100', 'hand1000', 'slope', 'wtd',\n",
    "        'landsat_1', 'landsat_2', 'landsat_3', 'landsat_4',\n",
    "        'landsat_7', 'NDVI']\n",
    "\n",
    "X = peatmat_data_proc.normalize_data(X, fields_to_transform)\n",
    "\n",
    "# Convert to tensor\n",
    "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32).to(device)\n",
    "\n",
    "# Apply the model\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fff0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def save_dataset_mat(output_dir, filename, X, y_pred_rs, outfmt=\"mat\"):\n",
    "    file_path = f\"{output_dir}/{filename}\"\n",
    "    with h5py.File(file_path, 'w') as f:\n",
    "        f.create_dataset('input', data=X.T)\n",
    "        f.create_dataset('target', data=y_pred_rs.cpu().detach().numpy().T)\n",
    "        f.close()\n",
    "    print(f\"Dataset saved to {file_path}\")\n",
    "\n",
    "# Assuming X and y_pred_rs are PyTorch tensors on GPU\n",
    "# Example usage\n",
    "\n",
    "filename = file_to_predict.split(\"/\")[-1].split(\".\")[0]\n",
    "outfilename = f\"{filename}_pred.mat\"\n",
    "save_dataset_mat(\"../outputs\", outfilename, X.to_numpy(), y_pred, outfmt=\"mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to dataframe\n",
    "y_pred_df = pd.DataFrame(y_pred.cpu().numpy(), columns=['peatland'])\n",
    "y_pred_df.columns = y_fields\n",
    "# Plot in two subplots the prediction and the ground truth\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].scatter(lat, lon, c=y['peatland'], cmap='viridis')\n",
    "ax[0].set_title(\"Ground truth\")\n",
    "ax[0].set_xlabel(\"Latitude\")\n",
    "ax[0].set_ylabel(\"Longitude\")\n",
    "ax[0].grid(True)\n",
    "# add the colorbar\n",
    "cbar = plt.colorbar(ax[0].scatter(lat, lon, c=y['peatland'], cmap='viridis'))\n",
    "# set limit of the colorbar\n",
    "cbar.set_label('Peatland')\n",
    "\n",
    "# Plot the prediction\n",
    "ax[1].scatter(lat, lon, c=y_pred_df['peatland'], cmap='viridis')\n",
    "ax[1].set_title(\"Prediction\")\n",
    "ax[1].set_xlabel(\"Latitude\")\n",
    "ax[1].set_ylabel(\"Longitude\")\n",
    "ax[1].grid(True)\n",
    "# add the colorbar\n",
    "cbar = plt.colorbar(ax[1].scatter(lat, lon, c=y_pred_df['peatland'], cmap='viridis'))\n",
    "cbar.set_label('Peatland')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# remove .mat extension from the filename\n",
    "file_to_predict = file_to_predict.split(\".\")[0]\n",
    "plt.savefig(f\"prediction_ground_truth_{file_to_predict}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
